#!/usr/bin/python3

# Simple Link Checker by Hanno Boeck, https://hboeck.de/
# License: 0BSD

# TODO:
# Don't double-check fragment-urls (#foo)

import argparse
import time
import urllib.parse

import bs4
import urllib3


def DEBUG(dbgmsg):
    if args.debug:
        print(f"DEBUG {dbgmsg}")


parser = argparse.ArgumentParser()
parser.add_argument("url", help="urls to scan")
parser.add_argument(
    "-m", "--maxdepth", default="3", type=int, help="Maximum depth on start domain"
)
parser.add_argument("-q", "--quiet", action="store_true", help="Only output findings")
parser.add_argument("-i", "--ignore", help="File with URLs to ignore")
parser.add_argument("-s", "--sleep", type=int, help="Seconds between requests")
parser.add_argument("--ignore403", action="store_true", help="Ignore 403 errors")
parser.add_argument(
    "--block", help="File with strings to consider suspicious, e.g. sedo"
)
parser.add_argument(
    "-d", "--debug", action="store_true", help="Show detailed debugging info"
)
args = parser.parse_args()

ignore = []
if args.ignore:
    with open(args.ignore) as f:
        ignore = [s.strip() for s in f.readlines()]
        DEBUG(f"Ignorelist: {ignore}")

block = []
if args.block:
    with open(args.block) as f:
        block = [s.strip() for s in f.readlines()]

if not args.quiet:
    print(f"Starting with {args.url}")


itemlist = [{"url": args.url, "topurl": False, "depth": 0}]
urllist = [args.url]

roothost = urllib.parse.urlparse(args.url).netloc
DEBUG(f"roothost {roothost}")

user_agent = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                  "(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}
poolmanager = urllib3.PoolManager(10, cert_reqs="CERT_REQUIRED", headers=user_agent)

warn = 0

for item in itemlist:
    if item["url"] in ignore:
        DEBUG(f"Ignoring {item['url']}")
        continue

    DEBUG(f"Processing {item['url']} depth {item['depth']}")
    item["processed"] = True

    if urllib.parse.urlparse(item["url"]).scheme in ["mailto", "javascript"]:
        # TODO: Could check if mail address is well formed
        DEBUG(f"Not parsing {item['url']}, different protocol")
        continue

    if args.sleep:
        time.sleep(args.sleep)
    try:
        r = poolmanager.request("GET", item["url"])
    except Exception as e:
        warn += 1
        print(
            f"Error (Exception {e}) with {item['url']} - linked from {item['topurl']}"
        )
        continue
    if r.status >= 400:
        if not args.ignore403 or r.status != 403:
            warn += 1
            print(
                f"Error (HTTP Code {r.status}) with {item['url']} - linked from {item['topurl']}"
            )
        continue
    DEBUG(f"{item['url']} returned HTTP Code {r.status}")

    if item["url"].startswith("http://") and r.geturl().startswith("https://"):
        if item["url"].replace("http://", "https://") == r.geturl():
            warn += 1
            print(
                f"HTTPS redirect (direct): {item['url']} -> {r.geturl()} from {item['topurl']}"
            )
        else:
            warn += 1
            print(
                f"HTTPS redirect (indirect): {item['url']} -> {r.geturl()} from {item['topurl']}"
            )

    for bl in block:
        if bl in r.data.decode(errors="ignore"):
            warn += 1
            print(f"BLOCKED string in {item['url']} - linked from {item['topurl']}")

    headers = {k.lower(): v for k, v in r.headers.items()}
    if not (
        "content-type" in headers and headers["content-type"].startswith("text/html")
    ):
        DEBUG(f"Not parsing {item['url']}, not text/html")
        continue
    if item["depth"] >= args.maxdepth:
        DEBUG(f"Not parsing {item['url']}, maxdepth reached")
        continue
    if urllib.parse.urlparse(item["url"]).netloc != roothost:
        DEBUG(f"Not parsing {item['url']}, different host")
        continue

    p = bs4.BeautifulSoup(r.data, "html.parser")

    srcs = []
    for tag in p.findAll(attrs={"src": True}):
        srcs.append(urllib.parse.urljoin(item["url"], tag["src"]))
    for tag in p.findAll(attrs={"href": True}):
        srcs.append(urllib.parse.urljoin(item["url"], tag["href"]))
    srcs = set(srcs)

    for s in srcs:
        if s in urllist:
            DEBUG(f"Already know {s}, skipping")
            continue
        newitem = {}
        newitem["url"] = s
        newitem["topurl"] = item["url"]
        newitem["processed"] = False
        newitem["depth"] = item["depth"] + 1
        itemlist.append(newitem)
        urllist.append(s)

poolmanager.clear()

if not args.quiet:
    print(f"Scanned {len(itemlist)} URLs, {warn} warnings")
