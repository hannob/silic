#!/usr/bin/python3

# Simple Link Checker by Hanno Boeck, https://hboeck.de/
# License: 0BSD

# TODO:
# Don't double-check fragment-urls (#foo)

import argparse
import urllib.parse
import urllib3
import bs4


def DEBUG(dbgmsg):
    if args.debug:
        print("DEBUG %s" % dbgmsg)


parser = argparse.ArgumentParser()
parser.add_argument("url", help="urls to scan")
parser.add_argument(
    "-m", "--maxdepth", default="3", type=int, help="Maximum depth on start domain"
)
parser.add_argument("-q", "--quiet", action="store_true", help="Only output findings")
parser.add_argument(
    "-d", "--debug", action="store_true", help="Show detailed debugging info"
)
args = parser.parse_args()

if not args.quiet:
    print("Starting with %s" % args.url)


itemlist = [{"url": args.url, "topurl": False, "depth": 0}]
urllist = [args.url]

roothost = urllib.parse.urlparse(args.url).netloc
DEBUG("roothost %s" % roothost)

user_agent = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}
poolmanager = urllib3.PoolManager(10, cert_reqs="CERT_REQUIRED", headers=user_agent)

for item in itemlist:
    DEBUG("Processing %s depth %i" % (item["url"], item["depth"]))
    item["processed"] = True

    if urllib.parse.urlparse(item["url"]).scheme in ["mailto", "javascript"]:
        # TODO: Could check if mail address is well formed
        DEBUG("Not parsing %s, different protocol" % item["url"])
        continue

    try:
        r = poolmanager.request("GET", item["url"])
    except Exception as e:
        print(
            f"Error (Exception {e}) with {item['url']} - linked from {item['topurl']}"
        )
        continue
    if r.status >= 400:
        print(
            f"Error (HTTP Code {r.status}) with {item['url']} - linked from {item['topurl']}"
        )
    DEBUG(f"{item['url']} returned HTTP Code {r.status}")

    headers = {k.lower(): v for k, v in r.headers.items()}
    if not (
        "content-type" in headers and headers["content-type"].startswith("text/html")
    ):
        DEBUG("Not parsing %s, not text/html" % item["url"])
        continue
    if item["depth"] >= args.maxdepth:
        DEBUG("Not parsing %s, maxdepth reached" % item["url"])
        continue
    if urllib.parse.urlparse(item["url"]).netloc != roothost:
        DEBUG("Not parsing %s, different host" % item["url"])
        continue

    p = bs4.BeautifulSoup(r.data, "html.parser")

    srcs = []
    for tag in p.findAll(attrs={"src": True}):
        srcs.append(urllib.parse.urljoin(item["url"], tag["src"]))
    for tag in p.findAll(attrs={"href": True}):
        srcs.append(urllib.parse.urljoin(item["url"], tag["href"]))
    srcs = set(srcs)

    for s in srcs:
        if s in urllist:
            DEBUG("Already know %s, skipping" % s)
            continue
        newitem = {}
        newitem["url"] = s
        newitem["topurl"] = item["url"]
        newitem["processed"] = False
        newitem["depth"] = item["depth"] + 1
        itemlist.append(newitem)
        urllist.append(s)

poolmanager.clear()

if not args.quiet:
    print("Scanned %i URLs" % len(itemlist))
